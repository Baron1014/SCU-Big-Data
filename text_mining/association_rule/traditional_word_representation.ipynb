{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.set_dictionary('dict/dict.txt.big')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from D:\\baron\\SCU\\巨資\\四上\\文字探勘\\association rule mining_HW\\src\\dict.txt.big ...\n",
      "Dumping model to file cache D:\\Users\\1070801\\AppData\\Local\\Temp\\jieba.u6e757094e45cdc3f7f6e976a892284ed.cache\n",
      "Loading model cost 1.800 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents ['文字探勘指的是從非結構化的文字中，萃取出有用的重要資訊或知識。', '資訊檢索指的是從資訊資源集合獲得與資訊需求相關的資訊資源的活動。', '搜尋可以基於全文或其他基於內容的索引。', '自然語言認知和理解，讓電腦把輸入的語言變成有意思的符號和關係，然後根據目的再處理。', '東吳大學是以教學為主、研究兼重之綜合大學，擁有外雙溪、城中兩校區，設有人文社會、外國語文、理、法、商及巨量資料管理等六個學院，共26個系所、學位學程。']\n",
      "segments ['文字 探勘 指 的 是從 非 結構化 的 文字 中 ， 萃取 出 有用 的 重要 資訊 或 知識 。', '資訊 檢索 指 的 是從 資訊 資源 集合 獲得 與 資訊 需求 相關 的 資訊 資源 的 活動 。', '搜尋 可以 基於 全文 或 其他 基於 內容 的 索引 。', '自然語言 認知 和 理解 ， 讓 電腦 把 輸入 的 語言 變成 有意思 的 符號 和 關係 ， 然後 根據 目的 再 處理 。', '東吳大學 是 以 教學 為主 、 研究 兼重 之 綜合大學 ， 擁有 外雙溪 、 城中 兩 校區 ， 設有 人文 社會 、 外國語文 、 理 、 法 、 商及 巨量 資料 管理 等 六個 學院 ， 共 26 個系 所 、 學位 學程 。']\n"
     ]
    }
   ],
   "source": [
    "documents = ['文字探勘指的是從非結構化的文字中，萃取出有用的重要資訊或知識。',\n",
    "             '資訊檢索指的是從資訊資源集合獲得與資訊需求相關的資訊資源的活動。',\n",
    "             '搜尋可以基於全文或其他基於內容的索引。',\n",
    "             '自然語言認知和理解，讓電腦把輸入的語言變成有意思的符號和關係，然後根據目的再處理。',\n",
    "             '東吳大學是以教學為主、研究兼重之綜合大學，擁有外雙溪、城中兩校區，設有人文社會、外國語文、理、法、商及巨量資料管理等六個學院，共26個系所、學位學程。']\n",
    "segments = [' '.join(jieba.cut(d)) for d in documents]\n",
    "print('documents', documents)\n",
    "print('segments', segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new documents ['詞嵌入是自然語言處理中語言模型與表徵學習技術的統稱。', '東吳大學掌握趨勢，瞄準未來，跨院系合作，首創成立「巨量資料管理學院」，培養符合企業需求的巨量資料領域專業人才，期許本學院成為國內巨量資料人才的培訓基地，無縫接軌校園的人才培訓與企業的實務需求。']\n",
      "new segments ['詞 嵌入 是 自然語言 處理 中 語言 模型 與 表徵 學習 技術 的 統稱 。', '東吳大學 掌握 趨勢 ， 瞄準 未來 ， 跨 院系 合作 ， 首創 成立 「 巨量 資料 管理 學院 」 ， 培養 符合 企業 需求 的 巨量 資料 領域 專業人才 ， 期許 本 學院 成為 國內 巨量 資料 人才 的 培訓基地 ， 無縫 接軌 校園 的 人才 培訓 與 企業 的 實務 需求 。']\n"
     ]
    }
   ],
   "source": [
    "new_documents = ['詞嵌入是自然語言處理中語言模型與表徵學習技術的統稱。',\n",
    "                 '東吳大學掌握趨勢，瞄準未來，跨院系合作，首創成立「巨量資料管理學院」，培養符合企業需求的巨量資料領域專業人才，期許本學院成為國內巨量資料人才的培訓基地，無縫接軌校園的人才培訓與企業的實務需求。']\n",
    "new_segments = [' '.join(jieba.cut(d)) for d in new_documents]\n",
    "print('new documents', new_documents)\n",
    "print('new segments', new_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Word Matrix Apporach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None)\n",
      "Number of vocabulary 61\n",
      "Vocabulary {'文字': 22, '探勘': 18, '是從': 23, '結構化': 43, '萃取': 46, '有用': 25, '重要': 56, '資訊': 54, '知識': 37, '檢索': 29, '資源': 53, '集合': 58, '獲得': 33, '需求': 60, '相關': 36, '活動': 30, '搜尋': 19, '可以': 8, '基於': 11, '全文': 4, '其他': 6, '內容': 3, '索引': 42, '自然語言': 45, '認知': 49, '理解': 34, '電腦': 59, '輸入': 55, '語言': 50, '變成': 51, '有意思': 24, '符號': 40, '關係': 57, '然後': 32, '根據': 28, '目的': 35, '處理': 47, '東吳大學': 26, '教學': 21, '為主': 31, '研究': 38, '兼重': 7, '綜合大學': 44, '擁有': 20, '外雙溪': 13, '城中': 10, '校區': 27, '設有': 48, '人文': 1, '社會': 39, '外國語文': 12, '商及': 9, '巨量': 17, '資料': 52, '管理': 41, '六個': 5, '學院': 16, '26': 0, '個系': 2, '學位': 14, '學程': 15}\n",
      "feature name ['26', '人文', '個系', '內容', '全文', '六個', '其他', '兼重', '可以', '商及', '城中', '基於', '外國語文', '外雙溪', '學位', '學程', '學院', '巨量', '探勘', '搜尋', '擁有', '教學', '文字', '是從', '有意思', '有用', '東吳大學', '校區', '根據', '檢索', '活動', '為主', '然後', '獲得', '理解', '目的', '相關', '知識', '研究', '社會', '符號', '管理', '索引', '結構化', '綜合大學', '自然語言', '萃取', '處理', '設有', '認知', '語言', '變成', '資料', '資源', '資訊', '輸入', '重要', '關係', '集合', '電腦', '需求']\n"
     ]
    }
   ],
   "source": [
    "# create bag-of-word model and fit the documents\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "print('vectorizer', vectorizer)\n",
    "\n",
    "vectorizer.fit(segments)\n",
    "print('Number of vocabulary', len(vectorizer.vocabulary_))\n",
    "print('Vocabulary', vectorizer.vocabulary_)\n",
    "print('feature name', vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse matrix   (0, 18)\t1\n",
      "  (0, 22)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 25)\t1\n",
      "  (0, 37)\t1\n",
      "  (0, 43)\t1\n",
      "  (0, 46)\t1\n",
      "  (0, 54)\t1\n",
      "  (0, 56)\t1\n",
      "  (1, 23)\t1\n",
      "  (1, 29)\t1\n",
      "  (1, 30)\t1\n",
      "  (1, 33)\t1\n",
      "  (1, 36)\t1\n",
      "  (1, 53)\t1\n",
      "  (1, 54)\t1\n",
      "  (1, 58)\t1\n",
      "  (1, 60)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 19)\t1\n",
      "  (2, 42)\t1\n",
      "  :\t:\n",
      "  (3, 59)\t1\n",
      "  (4, 0)\t1\n",
      "  (4, 1)\t1\n",
      "  (4, 2)\t1\n",
      "  (4, 5)\t1\n",
      "  (4, 7)\t1\n",
      "  (4, 9)\t1\n",
      "  (4, 10)\t1\n",
      "  (4, 12)\t1\n",
      "  (4, 13)\t1\n",
      "  (4, 14)\t1\n",
      "  (4, 15)\t1\n",
      "  (4, 16)\t1\n",
      "  (4, 17)\t1\n",
      "  (4, 20)\t1\n",
      "  (4, 21)\t1\n",
      "  (4, 26)\t1\n",
      "  (4, 27)\t1\n",
      "  (4, 31)\t1\n",
      "  (4, 38)\t1\n",
      "  (4, 39)\t1\n",
      "  (4, 41)\t1\n",
      "  (4, 44)\t1\n",
      "  (4, 48)\t1\n",
      "  (4, 52)\t1\n",
      "Dense matrix [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0\n",
      "  1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1]\n",
      " [0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1\n",
      "  0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 1 0]\n",
      " [1 1 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0\n",
      "  0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0]]\n",
      "Matrix shape (5, 61)\n"
     ]
    }
   ],
   "source": [
    "# transform the documents\n",
    "sparse_bag_of_words = vectorizer.transform(segments)\n",
    "print('Sparse matrix', sparse_bag_of_words)\n",
    "\n",
    "dense_bag_of_words = sparse_bag_of_words.toarray()\n",
    "print('Dense matrix', dense_bag_of_words)\n",
    "\n",
    "print('Matrix shape', dense_bag_of_words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dense matrix [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# transform the new documents\n",
    "new_sparse_bag_of_words = vectorizer.transform(new_segments)\n",
    "new_dense_bag_of_words = new_sparse_bag_of_words.toarray()\n",
    "print('New dense matrix', new_dense_bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf Matrix Apporach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None)\n",
      "Number of vocabulary 61\n",
      "Vocabulary {'文字': 22, '探勘': 18, '是從': 23, '結構化': 43, '萃取': 46, '有用': 25, '重要': 56, '資訊': 54, '知識': 37, '檢索': 29, '資源': 53, '集合': 58, '獲得': 33, '需求': 60, '相關': 36, '活動': 30, '搜尋': 19, '可以': 8, '基於': 11, '全文': 4, '其他': 6, '內容': 3, '索引': 42, '自然語言': 45, '認知': 49, '理解': 34, '電腦': 59, '輸入': 55, '語言': 50, '變成': 51, '有意思': 24, '符號': 40, '關係': 57, '然後': 32, '根據': 28, '目的': 35, '處理': 47, '東吳大學': 26, '教學': 21, '為主': 31, '研究': 38, '兼重': 7, '綜合大學': 44, '擁有': 20, '外雙溪': 13, '城中': 10, '校區': 27, '設有': 48, '人文': 1, '社會': 39, '外國語文': 12, '商及': 9, '巨量': 17, '資料': 52, '管理': 41, '六個': 5, '學院': 16, '26': 0, '個系': 2, '學位': 14, '學程': 15}\n",
      "feature name ['26', '人文', '個系', '內容', '全文', '六個', '其他', '兼重', '可以', '商及', '城中', '基於', '外國語文', '外雙溪', '學位', '學程', '學院', '巨量', '探勘', '搜尋', '擁有', '教學', '文字', '是從', '有意思', '有用', '東吳大學', '校區', '根據', '檢索', '活動', '為主', '然後', '獲得', '理解', '目的', '相關', '知識', '研究', '社會', '符號', '管理', '索引', '結構化', '綜合大學', '自然語言', '萃取', '處理', '設有', '認知', '語言', '變成', '資料', '資源', '資訊', '輸入', '重要', '關係', '集合', '電腦', '需求']\n"
     ]
    }
   ],
   "source": [
    "# create bag-of-word model and fit the documents\n",
    "vectorizer = CountVectorizer()\n",
    "print('vectorizer', vectorizer)\n",
    "\n",
    "vectorizer.fit(segments)\n",
    "print('Number of vocabulary', len(vectorizer.vocabulary_))\n",
    "print('Vocabulary', vectorizer.vocabulary_)\n",
    "print('feature name', vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense matrix [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 2 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0\n",
      "  1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 4 0 0 0 1 0 1]\n",
      " [0 0 0 1 1 0 1 0 1 0 0 2 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1\n",
      "  0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 1 0]\n",
      " [1 1 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0\n",
      "  0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0]]\n",
      "Matrix shape (5, 61)\n"
     ]
    }
   ],
   "source": [
    "# transform the documents\n",
    "dense_bag_of_words = vectorizer.transform(segments).toarray()\n",
    "print('Dense matrix', dense_bag_of_words)\n",
    "\n",
    "print('Matrix shape', dense_bag_of_words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the new documents\n",
    "new_sparse_bag_of_words = vectorizer.transform(new_segments)\n",
    "new_dense_bag_of_words = new_sparse_bag_of_words.toarray()\n",
    "print('New dense matrix', new_dense_bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf Matrix Apporach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None)\n",
      "Number of vocabulary 61\n",
      "Vocabulary {'文字': 22, '探勘': 18, '是從': 23, '結構化': 43, '萃取': 46, '有用': 25, '重要': 56, '資訊': 54, '知識': 37, '檢索': 29, '資源': 53, '集合': 58, '獲得': 33, '需求': 60, '相關': 36, '活動': 30, '搜尋': 19, '可以': 8, '基於': 11, '全文': 4, '其他': 6, '內容': 3, '索引': 42, '自然語言': 45, '認知': 49, '理解': 34, '電腦': 59, '輸入': 55, '語言': 50, '變成': 51, '有意思': 24, '符號': 40, '關係': 57, '然後': 32, '根據': 28, '目的': 35, '處理': 47, '東吳大學': 26, '教學': 21, '為主': 31, '研究': 38, '兼重': 7, '綜合大學': 44, '擁有': 20, '外雙溪': 13, '城中': 10, '校區': 27, '設有': 48, '人文': 1, '社會': 39, '外國語文': 12, '商及': 9, '巨量': 17, '資料': 52, '管理': 41, '六個': 5, '學院': 16, '26': 0, '個系': 2, '學位': 14, '學程': 15}\n",
      "feature name ['26', '人文', '個系', '內容', '全文', '六個', '其他', '兼重', '可以', '商及', '城中', '基於', '外國語文', '外雙溪', '學位', '學程', '學院', '巨量', '探勘', '搜尋', '擁有', '教學', '文字', '是從', '有意思', '有用', '東吳大學', '校區', '根據', '檢索', '活動', '為主', '然後', '獲得', '理解', '目的', '相關', '知識', '研究', '社會', '符號', '管理', '索引', '結構化', '綜合大學', '自然語言', '萃取', '處理', '設有', '認知', '語言', '變成', '資料', '資源', '資訊', '輸入', '重要', '關係', '集合', '電腦', '需求']\n"
     ]
    }
   ],
   "source": [
    "# create bag-of-word model and fit the documents\n",
    "vectorizer = TfidfVectorizer()\n",
    "print('vectorizer', vectorizer)\n",
    "\n",
    "vectorizer.fit(segments)\n",
    "print('Number of vocabulary', len(vectorizer.vocabulary_))\n",
    "print('Vocabulary', vectorizer.vocabulary_)\n",
    "print('feature name', vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense matrix [[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.29745795 0.         0.         0.         0.59491589 0.2399872\n",
      "  0.         0.29745795 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.29745795 0.         0.         0.         0.\n",
      "  0.         0.29745795 0.         0.         0.29745795 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.2399872  0.         0.29745795 0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.17578259\n",
      "  0.         0.         0.         0.         0.         0.21787799\n",
      "  0.21787799 0.         0.         0.21787799 0.         0.\n",
      "  0.21787799 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.43575598\n",
      "  0.70313036 0.         0.         0.         0.21787799 0.\n",
      "  0.21787799]\n",
      " [0.         0.         0.         0.31622777 0.31622777 0.\n",
      "  0.31622777 0.         0.31622777 0.         0.         0.63245553\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.31622777 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.31622777 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.26726124 0.         0.         0.         0.26726124 0.\n",
      "  0.         0.         0.26726124 0.         0.26726124 0.26726124\n",
      "  0.         0.         0.         0.         0.26726124 0.\n",
      "  0.         0.         0.         0.26726124 0.         0.26726124\n",
      "  0.         0.26726124 0.26726124 0.26726124 0.         0.\n",
      "  0.         0.26726124 0.         0.26726124 0.         0.26726124\n",
      "  0.        ]\n",
      " [0.20412415 0.20412415 0.20412415 0.         0.         0.20412415\n",
      "  0.         0.20412415 0.         0.20412415 0.20412415 0.\n",
      "  0.20412415 0.20412415 0.20412415 0.20412415 0.20412415 0.20412415\n",
      "  0.         0.         0.20412415 0.20412415 0.         0.\n",
      "  0.         0.         0.20412415 0.20412415 0.         0.\n",
      "  0.         0.20412415 0.         0.         0.         0.\n",
      "  0.         0.         0.20412415 0.20412415 0.         0.20412415\n",
      "  0.         0.         0.20412415 0.         0.         0.\n",
      "  0.20412415 0.         0.         0.         0.20412415 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]]\n",
      "Matrix shape (5, 61)\n"
     ]
    }
   ],
   "source": [
    "# transform the documents\n",
    "dense_bag_of_words = vectorizer.transform(segments).toarray()\n",
    "print('Dense matrix', dense_bag_of_words)\n",
    "\n",
    "print('Matrix shape', dense_bag_of_words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dense matrix [[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.57735027 0.         0.57735027\n",
      "  0.         0.         0.57735027 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.37796447 0.56694671\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.18898224 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.18898224\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.56694671 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.37796447]]\n"
     ]
    }
   ],
   "source": [
    "# transform the new documents\n",
    "new_sparse_bag_of_words = vectorizer.transform(new_segments)\n",
    "new_dense_bag_of_words = new_sparse_bag_of_words.toarray()\n",
    "print('New dense matrix', new_dense_bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
